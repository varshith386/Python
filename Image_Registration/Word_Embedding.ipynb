{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "directory = r\"C:\\Users\\hp\\OneDrive\\Desktop\\Pyhton\\Lex_Res\\dataset\\IN-Abs\\train-data\\judgement\"\n",
    "\n",
    "num_files= 50\n",
    "\n",
    "all_files = os.listdir(directory)\n",
    "\n",
    "selected_files = random.sample(all_files, num_files)\n",
    "\n",
    "output_directory = r\"C:\\Users\\hp\\OneDrive\\Desktop\\Pyhton\\Lex_Res\\dataset\\IN-Abs\\Lab-1_out\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for file_name in selected_files:\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    shutil.copy(file_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Most Common Words:\n",
      "section: 172951\n",
      "court: 161300\n",
      "high: 85603\n",
      "act: 84716\n",
      "order: 78759\n",
      "case: 74943\n",
      "would: 68493\n",
      "state: 67644\n",
      "may: 67037\n",
      "made: 66592\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "directory = r\"C:\\Users\\hp\\OneDrive\\Desktop\\Pyhton\\Lex_Res\\dataset\\IN-Abs\\train-data\\judgement\"\n",
    "\n",
    "def read_text_files(directory):\n",
    "    text = \"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text += file.read() + \" \"\n",
    "    return text\n",
    "\n",
    "student_answers_text = read_text_files(directory).lower()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_text = ' '.join(word for word in student_answers_text.split() if word not in stop_words)\n",
    "word_counts = Counter(filtered_text.split())\n",
    "most_common_words = word_counts.most_common(10)\n",
    "comm_words = [word for word, _ in most_common_words]\n",
    "\n",
    "\n",
    "print(\"10 Most Common Words:\")\n",
    "for word, count in most_common_words:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Scores (DistilBERT):\n",
      "[[0.20146713 0.24047151 0.17923944 ... 0.1234692  0.2279946  0.26077187]\n",
      " [0.21442153 0.23868933 0.1944493  ... 0.1361439  0.2516606  0.27307642]\n",
      " [0.2198585  0.24854828 0.18504138 ... 0.13990173 0.24965522 0.27396134]\n",
      " ...\n",
      " [0.20447534 0.24261372 0.1759761  ... 0.12676504 0.2408554  0.26743725]\n",
      " [0.20669276 0.24383539 0.17624404 ... 0.13335696 0.23383984 0.26146773]\n",
      " [0.2270654  0.2582651  0.19604611 ... 0.14498278 0.2713579  0.29332423]]\n"
     ]
    }
   ],
   "source": [
    "#DistilBERT\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def read_text_files(output_directory):\n",
    "    instances = []\n",
    "    for filename in os.listdir(output_directory):\n",
    "        filepath = os.path.join(output_directory, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read().strip()\n",
    "            instances.append(text)\n",
    "    return instances\n",
    "\n",
    "instances = read_text_files(output_directory)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def get_distilbert_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    embeddings = torch.mean(output.last_hidden_state, dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "seed_set_embeddings = np.array([get_distilbert_embedding(word) for word in comm_words])\n",
    "instances_embeddings = np.array([get_distilbert_embedding(instance) for instance in instances])\n",
    "cosine_similarities = cosine_similarity(instances_embeddings, seed_set_embeddings)\n",
    "\n",
    "\n",
    "print(\"Cosine Similarity Scores (DistilBERT):\")\n",
    "print(cosine_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d12ca627fa34f2dae671eb23ea1001c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Scores (RoBERTa):\n",
      "[[0.98738515 0.98761845 0.9869428  0.98756915 0.9873197  0.9864273\n",
      "  0.9870385  0.9867585  0.9873066  0.9873691 ]\n",
      " [0.9817469  0.98200715 0.98042893 0.9835068  0.98315895 0.9802439\n",
      "  0.9830696  0.98097354 0.9835026  0.98248255]\n",
      " [0.9888695  0.9895716  0.9868641  0.98917085 0.989339   0.9877735\n",
      "  0.98951846 0.98727435 0.9903146  0.9890726 ]\n",
      " [0.9872854  0.9888688  0.98796463 0.9879556  0.98944354 0.98793334\n",
      "  0.9889802  0.9898157  0.98932904 0.9887208 ]\n",
      " [0.99117035 0.9895297  0.9892731  0.9891218  0.9902905  0.9887982\n",
      "  0.99014986 0.99071336 0.99050415 0.9902965 ]\n",
      " [0.98890465 0.9874206  0.9860842  0.98816425 0.987254   0.98551023\n",
      "  0.98664296 0.98831505 0.9870095  0.98679644]\n",
      " [0.9899863  0.987856   0.98813766 0.9886341  0.98848426 0.9866415\n",
      "  0.9879627  0.9883324  0.98866683 0.98831004]\n",
      " [0.9911703  0.98952985 0.9892732  0.98912174 0.9902905  0.98879826\n",
      "  0.99015    0.99071354 0.99050415 0.9902965 ]\n",
      " [0.9888695  0.9895716  0.9868641  0.98917085 0.989339   0.9877735\n",
      "  0.98951846 0.98727435 0.9903146  0.9890726 ]\n",
      " [0.99015945 0.9894884  0.9891648  0.9901343  0.99003386 0.9884534\n",
      "  0.9895685  0.9883692  0.99091053 0.99026424]\n",
      " [0.9897797  0.9883481  0.98899454 0.9888173  0.988525   0.9869912\n",
      "  0.9883237  0.99005646 0.9894737  0.98901564]\n",
      " [0.9888695  0.9895716  0.9868641  0.98917085 0.989339   0.9877735\n",
      "  0.98951846 0.98727435 0.9903146  0.9890726 ]\n",
      " [0.9865023  0.9878197  0.9852625  0.9870958  0.98857623 0.9867445\n",
      "  0.9884155  0.98668116 0.9894383  0.98785025]\n",
      " [0.99009836 0.9891789  0.9873495  0.9886768  0.9891286  0.98793656\n",
      "  0.9894982  0.9907989  0.9898061  0.98953617]\n",
      " [0.98890465 0.9874206  0.9860842  0.98816425 0.987254   0.98551023\n",
      "  0.98664296 0.98831505 0.9870095  0.98679644]\n",
      " [0.98466295 0.9834055  0.98378265 0.98360044 0.98356473 0.98225677\n",
      "  0.98399    0.983254   0.983535   0.98412335]\n",
      " [0.9899863  0.987856   0.98813766 0.9886341  0.98848426 0.9866415\n",
      "  0.9879627  0.9883324  0.98866683 0.98831004]\n",
      " [0.98647314 0.9860621  0.98593044 0.986218   0.9867245  0.9842936\n",
      "  0.986573   0.987378   0.9878249  0.9877258 ]\n",
      " [0.98702055 0.9860685  0.9862802  0.98550403 0.9853537  0.9836875\n",
      "  0.98602456 0.98627627 0.98571205 0.9855722 ]\n",
      " [0.98890465 0.9874206  0.9860842  0.98816425 0.987254   0.98551023\n",
      "  0.98664296 0.98831505 0.9870095  0.98679644]\n",
      " [0.9888695  0.9895716  0.9868641  0.98917085 0.989339   0.9877735\n",
      "  0.98951846 0.98727435 0.9903146  0.9890726 ]\n",
      " [0.98466295 0.9834055  0.98378265 0.98360044 0.98356473 0.98225677\n",
      "  0.98399    0.983254   0.983535   0.98412335]\n",
      " [0.98890454 0.9874211  0.98608416 0.9881644  0.98725414 0.9855101\n",
      "  0.98664284 0.988315   0.9870095  0.98679644]\n",
      " [0.9911703  0.98952985 0.9892732  0.98912174 0.9902905  0.98879826\n",
      "  0.99015    0.99071354 0.99050415 0.9902965 ]\n",
      " [0.98729247 0.9871161  0.9855064  0.98641753 0.98776007 0.98572296\n",
      "  0.9875778  0.98851466 0.98933095 0.9880957 ]\n",
      " [0.98883915 0.98783445 0.98804915 0.98822826 0.98814005 0.98668027\n",
      "  0.9882135  0.9886984  0.98886776 0.98875034]\n",
      " [0.98943764 0.98883206 0.9871645  0.98879653 0.98925596 0.9873485\n",
      "  0.9894691  0.98978114 0.99005556 0.98976415]\n",
      " [0.9897797  0.9883481  0.98899454 0.9888173  0.988525   0.9869912\n",
      "  0.9883237  0.99005646 0.9894737  0.98901564]\n",
      " [0.9888695  0.9895716  0.9868641  0.98917085 0.989339   0.9877735\n",
      "  0.98951846 0.98727435 0.9903146  0.9890726 ]\n",
      " [0.98849535 0.98911756 0.98760617 0.98912615 0.98983794 0.9882796\n",
      "  0.9892916  0.9895056  0.9899451  0.98860264]\n",
      " [0.987781   0.9879988  0.9874269  0.98808956 0.9880677  0.9862234\n",
      "  0.987595   0.98944414 0.98956543 0.9876777 ]\n",
      " [0.9901595  0.9894885  0.98916477 0.9901346  0.99003375 0.9884532\n",
      "  0.9895685  0.988369   0.99091053 0.99026424]\n",
      " [0.98883915 0.98783445 0.98804915 0.98822826 0.98814005 0.98668027\n",
      "  0.9882135  0.9886984  0.98886776 0.98875034]\n",
      " [0.98943764 0.98883206 0.9871645  0.98879653 0.98925596 0.9873485\n",
      "  0.9894691  0.98978114 0.99005556 0.98976415]\n",
      " [0.990098   0.98917884 0.9873498  0.9886767  0.9891286  0.98793644\n",
      "  0.9894982  0.99079883 0.9898061  0.98953617]\n",
      " [0.9888695  0.9895716  0.9868641  0.98917085 0.989339   0.9877735\n",
      "  0.98951846 0.98727435 0.9903146  0.9890726 ]\n",
      " [0.98742765 0.9867647  0.98667496 0.9869009  0.9868739  0.98502433\n",
      "  0.98735577 0.9873982  0.98795545 0.9873429 ]\n",
      " [0.98890465 0.9874206  0.9860842  0.98816425 0.987254   0.98551023\n",
      "  0.98664296 0.98831505 0.9870095  0.98679644]\n",
      " [0.98531973 0.9814578  0.9812078  0.9840284  0.98118526 0.97957337\n",
      "  0.9822576  0.9832421  0.9830219  0.98274446]\n",
      " [0.98750806 0.988062   0.9850359  0.9888532  0.9877458  0.9861447\n",
      "  0.9876197  0.98570824 0.98890895 0.988239  ]\n",
      " [0.9874138  0.98749435 0.9871367  0.9877463  0.98849803 0.98684233\n",
      "  0.98752266 0.9864467  0.98727065 0.9870091 ]\n",
      " [0.98890465 0.9874206  0.9860842  0.98816425 0.987254   0.98551023\n",
      "  0.98664296 0.98831505 0.9870095  0.98679644]\n",
      " [0.9911703  0.98952985 0.9892732  0.98912174 0.9902905  0.98879826\n",
      "  0.99015    0.99071354 0.99050415 0.9902965 ]\n",
      " [0.9888695  0.9895716  0.9868641  0.98917085 0.989339   0.9877735\n",
      "  0.98951846 0.98727435 0.9903146  0.9890726 ]\n",
      " [0.98466116 0.9798071  0.98086864 0.97983897 0.9794818  0.9784112\n",
      "  0.98118925 0.9820399  0.9799217  0.98064154]\n",
      " [0.98380655 0.97972006 0.98148715 0.98137075 0.97970676 0.9781278\n",
      "  0.9798717  0.98257667 0.98054695 0.9812828 ]\n",
      " [0.98883915 0.98783445 0.98804915 0.98822826 0.98814005 0.98668027\n",
      "  0.9882135  0.9886984  0.98886776 0.98875034]\n",
      " [0.98380655 0.97972006 0.98148715 0.98137075 0.97970676 0.9781278\n",
      "  0.9798717  0.98257667 0.98054695 0.9812828 ]\n",
      " [0.9911703  0.98952985 0.9892732  0.98912174 0.9902905  0.98879826\n",
      "  0.99015    0.99071354 0.99050415 0.9902965 ]\n",
      " [0.98890465 0.9874206  0.9860842  0.98816425 0.987254   0.98551023\n",
      "  0.98664296 0.98831505 0.9870095  0.98679644]\n",
      " [0.98883915 0.98783445 0.98804915 0.98822826 0.98814005 0.98668027\n",
      "  0.9882135  0.9886984  0.98886776 0.98875034]\n",
      " [0.9888695  0.9895716  0.9868641  0.98917085 0.989339   0.9877735\n",
      "  0.98951846 0.98727435 0.9903146  0.9890726 ]\n",
      " [0.98395246 0.9848299  0.98412645 0.98465395 0.9852674  0.98328984\n",
      "  0.9852148  0.9841245  0.98618317 0.98553824]\n",
      " [0.9892833  0.9895674  0.9868825  0.9891996  0.99001205 0.9890719\n",
      "  0.9900051  0.99026346 0.99021643 0.989576  ]\n",
      " [0.9805939  0.9805014  0.9795697  0.9807638  0.9808757  0.97969717\n",
      "  0.9816796  0.978933   0.9807483  0.9817715 ]\n",
      " [0.97835505 0.9768803  0.97755575 0.9781368  0.9774349  0.9759934\n",
      "  0.9769723  0.97771454 0.977208   0.9776899 ]\n",
      " [0.9891736  0.9863877  0.986544   0.98623574 0.9860619  0.9850777\n",
      "  0.9866571  0.98741114 0.9872994  0.9871492 ]\n",
      " [0.9911703  0.98952985 0.9892732  0.98912174 0.9902905  0.98879826\n",
      "  0.99015    0.99071354 0.99050415 0.9902965 ]\n",
      " [0.9888695  0.9895716  0.9868641  0.98917085 0.989339   0.9877735\n",
      "  0.98951846 0.98727435 0.9903146  0.9890726 ]\n",
      " [0.98742765 0.9867647  0.98667496 0.9869009  0.9868739  0.98502433\n",
      "  0.98735577 0.9873982  0.98795545 0.9873429 ]\n",
      " [0.98380655 0.97972006 0.98148715 0.98137075 0.97970676 0.9781278\n",
      "  0.9798717  0.98257667 0.98054695 0.9812828 ]\n",
      " [0.9891736  0.9863877  0.986544   0.98623574 0.9860619  0.9850777\n",
      "  0.9866571  0.98741114 0.9872994  0.9871492 ]\n",
      " [0.9805939  0.9805014  0.9795697  0.9807638  0.9808757  0.97969717\n",
      "  0.9816796  0.978933   0.9807483  0.9817715 ]\n",
      " [0.98276174 0.9822388  0.98362833 0.9818024  0.98249066 0.98103184\n",
      "  0.98240274 0.98131895 0.9829332  0.98359567]\n",
      " [0.98750806 0.988062   0.9850359  0.9888532  0.9877458  0.9861447\n",
      "  0.9876197  0.98570824 0.98890895 0.988239  ]\n",
      " [0.98943764 0.98883206 0.9871645  0.98879653 0.98925596 0.9873485\n",
      "  0.9894691  0.98978114 0.99005556 0.98976415]\n",
      " [0.99017376 0.9909205  0.98948246 0.9905382  0.9916247  0.99000823\n",
      "  0.99181277 0.9918459  0.9920879  0.9913044 ]\n",
      " [0.98883915 0.98783445 0.98804915 0.98822826 0.98814005 0.98668027\n",
      "  0.9882135  0.9886984  0.98886776 0.98875034]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "file_path = r\"C:\\Users\\hp\\OneDrive\\Desktop\\Pyhton\\Lex_Res\\dataset\\IN-Abs\\Lab-1_out\"\n",
    "instances = file_path\n",
    "\n",
    "seed_set = ['section', 'court', 'high', 'act', 'order', 'case', 'would', 'state', 'may', 'made']\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def get_roberta_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    embeddings = torch.mean(output.last_hidden_state, dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "seed_set_embeddings = np.array([get_roberta_embedding(word) for word in seed_set])\n",
    "instances_embeddings = np.array([get_roberta_embedding(instance) for instance in instances])\n",
    "cosine_similarities_roberta = cosine_similarity(instances_embeddings, seed_set_embeddings)\n",
    "\n",
    "print(\"Cosine Similarity Scores (RoBERTa):\")\n",
    "print(cosine_similarities_roberta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfasttext\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the project dataset\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "instances = r\"C:\\Users\\hp\\OneDrive\\Desktop\\Pyhton\\Lex_Res\\dataset\\IN-Abs\\Lab-1_out\"\n",
    "\n",
    "seed_set = ['section', 'court', 'high', 'act', 'order', 'case', 'would', 'state', 'may', 'made']\n",
    "\n",
    "fasttext_model = fasttext.load_model('path_to_fasttext_model.bin') \n",
    "\n",
    "seed_set_embeddings_fasttext = np.array([fasttext_model.get_word_vector(word) for word in seed_set])\n",
    "instances_embeddings_fasttext = np.array([fasttext_model.get_sentence_vector(instance) for instance in instances])\n",
    "cosine_similarities_fasttext = cosine_similarity(instances_embeddings_fasttext, seed_set_embeddings_fasttext)\n",
    "\n",
    "print(\"Cosine Similarity Scores (FastText):\")\n",
    "print(cosine_similarities_fasttext)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
